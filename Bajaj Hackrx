{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12529265,"sourceType":"datasetVersion","datasetId":7909180}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the Kaggle-specific library to access our secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n# Retrieve our GitHub username and token\n# Make sure the labels 'GITHUB_USER' and 'GITHUB_TOKEN' match what you created\n# in the Kaggle Secrets menu.\nGITHUB_USER = user_secrets.get_secret(\"GITHUB_USER\")\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\n# This is the URL to your repository\n# Make sure to replace 'YourUsername' with your actual GitHub username\n# and 'bajaj-hackrx-llm-agent' with your repository name if it's different.\nGIT_REPO_URL = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/bajaj-hackrx-project.git\"\n\n# --- Clone the repository ---\n# The '!' tells the notebook to run this as a command line command\n# We are cloning the repository into a directory with the same name\n!git clone {GIT_REPO_URL}\n\n# --- Verify the setup ---\n# Change our current directory to be inside our new project folder\n%cd bajaj-hackrx-project\n\nprint(\"✅ Setup Complete! Your GitHub repository is now connected.\")\nprint(\"\\nCurrent files in your project:\")\n!ls -F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T02:08:20.392991Z","iopub.execute_input":"2025-07-21T02:08:20.394067Z","iopub.status.idle":"2025-07-21T02:08:21.192988Z","shell.execute_reply.started":"2025-07-21T02:08:20.394017Z","shell.execute_reply":"2025-07-21T02:08:21.191584Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"fatal: destination path 'bajaj-hackrx-project' already exists and is not an empty directory.\n/kaggle/working/bajaj-hackrx-project\n✅ Setup Complete! Your GitHub repository is now connected.\n\nCurrent files in your project:\nLICENSE  README.md\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Step 1: Install the required library for PDF processing ---\n# We only need to run this once.\n!pip install pypdf\n\nimport os\nimport pandas as pd\nfrom pypdf import PdfReader\n\n# --- Step 2: Define a function to extract text from a PDF ---\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Opens a PDF file and extracts all text content from it.\n    \n    Args:\n        pdf_path (str): The full path to the PDF file.\n        \n    Returns:\n        str: The concatenated text from all pages of the PDF.\n    \"\"\"\n    try:\n        reader = PdfReader(pdf_path)\n        text = \"\"\n        for page in reader.pages:\n            text += page.extract_text() + \"\\n\" # Add a newline between pages\n        return text\n    except Exception as e:\n        print(f\"Error reading {pdf_path}: {e}\")\n        return None\n\n# --- Step 3: Process all documents in our dataset ---\n# IMPORTANT: Make sure your Kaggle dataset is named 'bajaj-policy-docs'\n# If you named it something else, change the path here.\ndocs_path = \"/kaggle/input/bajaj-policy-docs/\"\nprocessed_docs = []\n\n# Check if the directory exists\nif os.path.exists(docs_path):\n    print(f\"Found document directory: {docs_path}\")\n    # Loop through all files in the directory\n    for filename in os.listdir(docs_path):\n        if filename.lower().endswith(\".pdf\"):\n            file_path = os.path.join(docs_path, filename)\n            print(f\"Processing document: {filename}...\")\n            \n            # Extract the text\n            document_text = extract_text_from_pdf(file_path)\n            \n            if document_text:\n                processed_docs.append({\n                    \"document_name\": filename,\n                    \"text_content\": document_text\n                })\nelse:\n    print(f\"Error: Directory not found at {docs_path}\")\n    print(\"Please ensure you have uploaded the sample documents as a Kaggle dataset.\")\n\n# --- Step 4: Display the results ---\n# Create a pandas DataFrame to neatly display our extracted text\ndocs_df = pd.DataFrame(processed_docs)\n\nif not docs_df.empty:\n    print(\"\\n✅ Document processing complete.\")\n    print(f\"Successfully processed {len(docs_df)} documents.\")\n    \n    # Print the first few lines of the first document to verify\n    print(\"\\n--- Sample content from the first document ---\")\n    print(docs_df.iloc[0]['text_content'][:500])\n    print(\"-------------------------------------------\")\nelse:\n    print(\"\\n⚠️ No documents were processed. Please check the file paths and formats.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:56:59.721740Z","iopub.execute_input":"2025-07-21T14:56:59.722193Z","iopub.status.idle":"2025-07-21T14:57:18.614833Z","shell.execute_reply.started":"2025-07-21T14:56:59.722165Z","shell.execute_reply":"2025-07-21T14:57:18.613820Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.7.0)\nFound document directory: /kaggle/input/bajaj-policy-docs/\nProcessing document: BAJHLIP23020V012223.pdf...\nProcessing document: EDLHLGA23009V012223.pdf...\nProcessing document: ICIHLIP22012V012223.pdf...\nProcessing document: CHOTGDP23004V012223.pdf...\nProcessing document: HDFHLIP23024V072223.pdf...\n\n✅ Document processing complete.\nSuccessfully processed 5 documents.\n\n--- Sample content from the first document ---\n    \n \n   \n \nUIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 1 \n \n \nBajaj Allianz General Insurance Co. Ltd.                       \nBajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 \nFor more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or \nCall at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) \nIssuing Office: \n \nGLOBAL HEALTH CARE \n \n \nPolicy Wordings \n \nUIN- BAJHLIP2\n-------------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Chunking\n!pip install langchain\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport pandas as pd\n\n# --- Prerequisite: Assumes 'docs_df' from the previous step is in memory ---\n\nif 'docs_df' in locals() and not docs_df.empty:\n    # --- Step 2: Define our text splitter ---\n    # This splitter will try to break text by paragraphs (\"\\n\\n\"), then by lines (\"\\n\"),\n    # and finally by sentences or words to respect the document's structure.\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,  # The maximum size of each chunk (in characters)\n        chunk_overlap=100, # The number of characters to overlap between chunks\n        length_function=len,\n    )\n\n    # --- Step 3: Process all documents and create chunks ---\n    all_chunks = []\n\n    print(\"Starting to chunk documents...\")\n    # Iterate through each document in our DataFrame\n    for index, row in docs_df.iterrows():\n        document_name = row['document_name']\n        document_text = row['text_content']\n        \n        # Use the splitter to create chunks from the document's text\n        chunks = text_splitter.split_text(document_text)\n        \n        # Add each chunk to our list with its source document\n        for i, chunk_text in enumerate(chunks):\n            all_chunks.append({\n                \"document_name\": document_name,\n                \"chunk_id\": f\"{document_name}_chunk_{i}\",\n                \"chunk_text\": chunk_text\n            })\n    \n    # --- Step 4: Display the results ---\n    # Create a new DataFrame from our list of chunks\n    chunks_df = pd.DataFrame(all_chunks)\n\n    if not chunks_df.empty:\n        print(\"\\n✅ Document chunking complete.\")\n        print(f\"Created {len(chunks_df)} chunks from {len(docs_df)} documents.\")\n        \n        # Print a sample chunk to verify\n        print(\"\\n--- Sample Chunk ---\")\n        print(f\"Source Document: {chunks_df.iloc[0]['document_name']}\")\n        print(\"--------------------\")\n        print(chunks_df.iloc[0]['chunk_text'])\n        print(\"--------------------\")\n    else:\n        print(\"\\n⚠️ No chunks were created.\")\n\nelse:\n    print(\"Error: 'docs_df' not found or is empty. Please run the previous document processing step first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:57:21.810176Z","iopub.execute_input":"2025-07-21T14:57:21.810762Z","iopub.status.idle":"2025-07-21T14:57:25.656043Z","shell.execute_reply.started":"2025-07-21T14:57:21.810728Z","shell.execute_reply":"2025-07-21T14:57:25.654875Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.4)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\nStarting to chunk documents...\n\n✅ Document chunking complete.\nCreated 911 chunks from 5 documents.\n\n--- Sample Chunk ---\nSource Document: BAJHLIP23020V012223.pdf\n--------------------\nUIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 1 \n \n \nBajaj Allianz General Insurance Co. Ltd.                       \nBajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 \nFor more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or \nCall at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) \nIssuing Office: \n \nGLOBAL HEALTH CARE \n \n \nPolicy Wordings \n \nUIN- BAJHLIP23020V012223 \nSECTION A) PREAMBLE \n \nWhereas the Insured described in the Policy Schedule hereto (hereinafter called the ‘Insured’  or “Policyholder” or \n“Insured Person”) has made to Bajaj Allianz General Insurance Company Limited (hereinafter called the “Company” \nor “Insurer” or “Insurance Company”) a proposal or Proposal as mentioned in the transcript of the Proposal, which \nshall be the basis of this Contract and is deemed to be incorporated herein, containing certain undertakings ,\n--------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Creating Embeddings(Vector Data)\n# --- Step 1: Install the required library for creating embeddings ---\n# sentence-transformers is a powerful library from HuggingFace\n!pip install -U sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\n\n# --- Prerequisite: Assumes 'chunks_df' from the previous step is in memory ---\n\nif 'chunks_df' in locals() and not chunks_df.empty:\n    # --- Step 2: Load a pre-trained embedding model ---\n    # 'all-MiniLM-L6-v2' is a popular, fast, and high-quality model.\n    # The first time you run this, it will download the model (a few hundred MB).\n    print(\"Loading the embedding model...\")\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    print(\"✅ Model loaded successfully.\")\n\n    # --- Step 3: Create the embeddings for each chunk ---\n    # We will process the text in batches for efficiency.\n    # Get the list of all chunk texts\n    chunk_texts = chunks_df['chunk_text'].tolist()\n\n    print(f\"\\nCreating embeddings for {len(chunk_texts)} chunks... (This may take a few minutes)\")\n    # The model.encode() function takes a list of texts and returns a list of embeddings\n    embeddings = model.encode(chunk_texts, show_progress_bar=True)\n    \n    # Add the embeddings to our DataFrame\n    chunks_df['embedding'] = list(embeddings)\n\n    # --- Step 4: Display the results ---\n    print(\"\\n✅ Embeddings created successfully.\")\n    print(\"Sample of the chunks DataFrame with embeddings:\")\n    \n    # Show the first few rows, including the new 'embedding' column\n    print(chunks_df.head())\n    \n    # Let's inspect one of the embeddings\n    print(\"\\n--- Sample Embedding ---\")\n    sample_embedding = chunks_df.iloc[0]['embedding']\n    print(f\"Embedding Type: {type(sample_embedding)}\")\n    print(f\"Embedding Length (Dimensions): {len(sample_embedding)}\")\n    print(f\"First 5 values: {sample_embedding[:5]}\")\n    print(\"----------------------\")\n\nelse:\n    print(\"Error: 'chunks_df' not found or is empty. Please run the previous document chunking step first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T14:59:26.897332Z","iopub.execute_input":"2025-07-21T14:59:26.898154Z","iopub.status.idle":"2025-07-21T15:02:45.099393Z","shell.execute_reply.started":"2025-07-21T14:59:26.898117Z","shell.execute_reply":"2025-07-21T15:02:45.098302Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.0.0\n","output_type":"stream"},{"name":"stderr","text":"2025-07-21 15:01:15.031688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753110075.237572      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753110075.294679      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading the embedding model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192641a595274f098e30380b6af8baae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8193923f650a445f8e33fd8452a85352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648d34c287314c978e084ca93f1c2b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f262dc8354d143738e410468e5adaea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ac71520abc42a391aa1451d883d01c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb6c6a3d8a144879aed8748e30609db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a2c138d44946fcab8538dfb712ca31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b346a179d7494e11a3dec9e073f8fe87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60c1f8721d1e45639285966084e99c73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8b6d4304ac487396fbbe1f12e01155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd27abe223a64eb7958b44ecbd441a94"}},"metadata":{}},{"name":"stdout","text":"✅ Model loaded successfully.\n\nCreating embeddings for 911 chunks... (This may take a few minutes)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aed985d2ef1421699ea1a203b0aa41f"}},"metadata":{}},{"name":"stdout","text":"\n✅ Embeddings created successfully.\nSample of the chunks DataFrame with embeddings:\n             document_name                         chunk_id  \\\n0  BAJHLIP23020V012223.pdf  BAJHLIP23020V012223.pdf_chunk_0   \n1  BAJHLIP23020V012223.pdf  BAJHLIP23020V012223.pdf_chunk_1   \n2  BAJHLIP23020V012223.pdf  BAJHLIP23020V012223.pdf_chunk_2   \n3  BAJHLIP23020V012223.pdf  BAJHLIP23020V012223.pdf_chunk_3   \n4  BAJHLIP23020V012223.pdf  BAJHLIP23020V012223.pdf_chunk_4   \n\n                                          chunk_text  \\\n0  UIN- BAJHLIP23020V012223                      ...   \n1  declarations, information/particulars and stat...   \n2  permits. If any word starts with Capital alpha...   \n3  Indian Medicine/Central Council for Homeopathy...   \n4  applicable and having facilities for carrying ...   \n\n                                           embedding  \n0  [-0.05861414, 0.046383865, -0.013737356, -0.00...  \n1  [-0.046450872, 0.06120499, -0.009025839, -0.02...  \n2  [0.012981687, -0.023778314, 0.0009084417, 0.02...  \n3  [-0.043896962, 0.00039167152, -0.043071907, 0....  \n4  [-0.017694758, -0.005337816, -0.019131795, -0....  \n\n--- Sample Embedding ---\nEmbedding Type: <class 'numpy.ndarray'>\nEmbedding Length (Dimensions): 384\nFirst 5 values: [-0.05861414  0.04638387 -0.01373736 -0.00625373 -0.00076511]\n----------------------\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Step 1: Install the required library for the vector database ---\n!pip install faiss-cpu\n\nimport faiss\nimport numpy as np\nimport pandas as pd\n\n# --- Prerequisite: Assumes 'chunks_df' from the previous step is in memory ---\n\nif 'chunks_df' in locals() and not chunks_df.empty and 'embedding' in chunks_df.columns:\n    # --- Step 2: Prepare the embeddings for FAISS ---\n    # FAISS requires the embeddings to be in a specific format (a 2D numpy array of type float32).\n    print(\"Converting embeddings to a FAISS-compatible format...\")\n    embeddings = np.array(chunks_df['embedding'].tolist()).astype('float32')\n    \n    # Check the shape of our embeddings matrix\n    print(f\"Embeddings matrix shape: {embeddings.shape}\") # Should be (num_chunks, embedding_dimension)\n\n    # --- Step 3: Create the FAISS Index ---\n    # Get the dimension of our embeddings (e.g., 384 for the 'all-MiniLM-L6-v2' model)\n    d = embeddings.shape[1]\n    \n    # We will use a simple, exact-search index called 'IndexFlatL2'.\n    # This index performs an exhaustive search, which is perfect for our scale.\n    print(f\"Creating a FAISS index with dimension {d}...\")\n    index = faiss.IndexFlatL2(d)\n    \n    # --- Step 4: Add the embeddings to the index ---\n    print(f\"Adding {len(embeddings)} embeddings to the index...\")\n    index.add(embeddings)\n    \n    print(f\"✅ FAISS index created successfully. Total vectors in index: {index.ntotal}\")\n\n    # --- Step 5: Save the index and the chunk data for later use ---\n    # We need to save both the index itself and the dataframe that maps an index ID\n    # back to its original text content.\n    \n    # Save the FAISS index\n    faiss.write_index(index, \"policy_document_index.faiss\")\n    \n    # Save the chunks dataframe (our mapping file)\n    # We drop the embedding column before saving to make the file smaller.\n    chunks_df.drop(columns=['embedding']).to_csv(\"policy_document_chunks.csv\", index=False)\n    \n    print(\"\\n✅ Knowledge base created and saved successfully.\")\n    print(\"Saved files: 'policy_document_index.faiss' and 'policy_document_chunks.csv'\")\n\nelse:\n    print(\"Error: 'chunks_df' with embeddings not found. Please run the previous steps first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:04:06.801831Z","iopub.execute_input":"2025-07-21T15:04:06.802254Z","iopub.status.idle":"2025-07-21T15:04:12.987476Z","shell.execute_reply.started":"2025-07-21T15:04:06.802219Z","shell.execute_reply":"2025-07-21T15:04:12.986569Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0.post1\nConverting embeddings to a FAISS-compatible format...\nEmbeddings matrix shape: (911, 384)\nCreating a FAISS index with dimension 384...\nAdding 911 embeddings to the index...\n✅ FAISS index created successfully. Total vectors in index: 911\n\n✅ Knowledge base created and saved successfully.\nSaved files: 'policy_document_index.faiss' and 'policy_document_chunks.csv'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#Retriever\n\nimport faiss\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n\n# --- Step 1: Load all our saved components ---\nprint(\"Loading the knowledge base and embedding model...\")\n\ntry:\n    # Load the FAISS index\n    index = faiss.read_index(\"policy_document_index.faiss\")\n    \n    # Load the chunk data that maps index IDs to text\n    chunks_df = pd.read_csv(\"policy_document_chunks.csv\")\n    \n    # Load the sentence transformer model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    print(\"✅ Knowledge base and model loaded successfully.\")\n    print(f\"Index contains {index.ntotal} vectors.\")\n\nexcept Exception as e:\n    print(f\"Error loading files: {e}\")\n    print(\"Please ensure 'policy_document_index.faiss' and 'policy_document_chunks.csv' are in the correct directory.\")\n\n# --- Step 2: Define the search function ---\ndef search_documents(query, top_k=3):\n    \"\"\"\n    Performs a semantic search for a given query against the FAISS index.\n    \n    Args:\n        query (str): The user's search query.\n        top_k (int): The number of top results to return.\n        \n    Returns:\n        pd.DataFrame: A DataFrame containing the top_k most relevant chunks.\n    \"\"\"\n    if 'index' not in globals():\n        print(\"Error: FAISS index not loaded.\")\n        return None\n        \n    print(f\"\\nSearching for: '{query}'...\")\n    # 1. Create an embedding for the user's query\n    query_embedding = model.encode([query], convert_to_tensor=False).astype('float32')\n    \n    # 2. Perform the search in our FAISS index\n    # The search function returns distances and the indices (IDs) of the top_k results\n    distances, indices = index.search(query_embedding, top_k)\n    \n    # 3. Retrieve the results\n    # The 'indices' is a 2D array, so we take the first row\n    results_indices = indices[0]\n    \n    # Get the corresponding chunks from our dataframe\n    results_df = chunks_df.iloc[results_indices].copy()\n    results_df['similarity_score'] = 1 - distances[0] # Convert L2 distance to a similarity score\n    \n    return results_df\n\n# --- Step 3: Test our search function ---\nif 'index' in globals():\n    # Let's test with a sample query from the problem statement\n    sample_query = \"Is knee surgery covered?\"\n    \n    search_results = search_documents(sample_query)\n    \n    if search_results is not None:\n        print(\"\\n--- Top Search Results ---\")\n        # Display the results\n        for index, row in search_results.iterrows():\n            print(f\"Document: {row['document_name']}\")\n            print(f\"Similarity: {row['similarity_score']:.4f}\")\n            print(\"---\")\n            print(row['chunk_text'])\n            print(\"\\n--------------------------\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:06:02.078206Z","iopub.execute_input":"2025-07-21T15:06:02.078649Z","iopub.status.idle":"2025-07-21T15:06:03.640037Z","shell.execute_reply.started":"2025-07-21T15:06:02.078615Z","shell.execute_reply":"2025-07-21T15:06:03.639084Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"Loading the knowledge base and embedding model...\n✅ Knowledge base and model loaded successfully.\nIndex contains 911 vectors.\n\nSearching for: 'Is knee surgery covered?'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de7538fe85547e9b80326f31d909b21"}},"metadata":{}},{"name":"stdout","text":"\n--- Top Search Results ---\nDocument: HDFHLIP23024V072223.pdf\nSimilarity: -0.0402\n---\nfractures (excluding hairline fractures) and dislocations of the \nmandible and extremities. \n16) Dental treatment and surgery of any kind, unless requiring \nHospitalisation  \n17) Any non medical expenses mentioned in List 1 of Annexure I  \n18) Treatment rendered by a Medical Practitioner which is outside \nhis discipline or the discipline for which he is licensed. \n19) Treatments rendered by a Medical Practitioner who is a \nmember of the Insured Person’s family or stays with him, however \nproven material costs are eligible for reimbursement in accordance \nwith the applicable cover. \n20) Any treatment or part of a treatment that is not of a \nreasonable charge and not Medically Necessary.  \n21) Drugs or treatments which are not supported by a \nprescription. \n22) Any specific time bound or lifetime exclusion(s) applied by Us \nand specified in the Schedule and accepted by the insured. \n23) Admission for administration of Intra-articular or Intra-lesional\n\n--------------------------\n\nDocument: BAJHLIP23020V012223.pdf\nSimilarity: -0.0937\n---\nexcluded.  \n \n5) Rest Cure, rehabilitation and respite care (Code-Excl05) \na. Expenses related to any admission primarily for enforced bed rest and not for receiving treatment. This also \nincludes: \nb. Custodial care either at home or in a nursing facility for personal care such as help with activities of daily living \nsuch as bathing, dressing, moving around either by skilled nurses or assistant or non-skilled persons. \nc. Any services for people who are terminally ill to address physical, social, emotional and spiritual needs. \n \n6) Obesity/Weight Control (Code-Excl06) \na. Expenses related to the surgical treatment of obesity that does not fulfil all the below conditions: \nb. Surgery to be conducted is upon the advice of the Doctor \nc. The surgery/Procedure conducted should be supported by clinical protocols \nd. The member has to be 18 years of age or older and \ne. Body Mass Index (BMI); \ni. greater than or equal to 40 or\n\n--------------------------\n\nDocument: BAJHLIP23020V012223.pdf\nSimilarity: -0.1136\n---\n86   Electron Therapy 285  Arthroscopic meniscus repair \n87  TSET-Total Electron Skin Therapy 286  Haemarthrosis knee- lavage \n88  Extracorporeal Irradiation of  Blood Products 287  Abscess  knee joint drainage  \n89  Telecobalt Therapy 288  Carpal tunnel release \n90  Telecesium Therapy 289  Closed reduction of minor dislocation \n91   External mould Brachytherapy 290  Repair of knee cap tendon \n92   Interstitial Brachytherapy 291  ORIF with K wire fixation- small bones \n93  Intracavity Brachytherapy 292  Release of midfoot joint \n94  3D Brachytherapy 293  ORIF with plating- Small long bones \n95  Implant Brachytherapy 294  Implant removal minor \n96  Intravesical Brachytherapy 295  K wire removal \n97  Adjuvant Radiotherapy 296  POP application \n98   Afterloading Catheter Brachytherapy 297  Closed reduction and external fixation \n99  Conditioning Radiothearpy for BMT 298  Arthrotomy Hip joint  \n100  Extracorporeal Irradiation to the Homologous \nBone grafts 299  Syme's amputation\n\n--------------------------\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# --- Prerequisite: This code assumes you are in an environment with access to the Gemini API ---\n# In a local setup, you would use libraries like `google-generativeai`.\n# In this environment, we will simulate the API call structure.\n\n# --- Step 1: Define the LLM-powered parser function ---\ndef parse_query_with_llm(raw_query):\n    \"\"\"\n    Uses a Large Language Model to parse a raw query and extract key entities into a JSON format.\n    \n    Args:\n        raw_query (str): The user's unstructured query.\n        \n    Returns:\n        dict: A dictionary containing the extracted entities.\n    \"\"\"\n    print(f\"\\nParsing query with LLM: '{raw_query}'...\")\n    \n    # This is the core of the function: a carefully designed prompt.\n    # It tells the LLM its role, the task, the desired output format, and provides examples.\n    prompt = f\"\"\"\n    You are an expert data extraction agent for an insurance company.\n    Your task is to parse a user's query and extract the key details into a structured JSON object.\n\n    The possible fields to extract are:\n    - \"age\" (integer)\n    - \"gender\" (string, either \"male\", \"female\", or \"unspecified\")\n    - \"procedure\" (string, the medical procedure mentioned)\n    - \"location\" (string, the city or location mentioned)\n    - \"policy_duration_months\" (integer, the age of the policy in months)\n    - \"core_question\" (string, a concise summary of the user's main question for semantic search)\n\n    If a piece of information is not present in the query, do not include its key in the JSON output.\n\n    Here are some examples:\n\n    Query: \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\"\n    Output:\n    {{\n      \"age\": 46,\n      \"gender\": \"male\",\n      \"procedure\": \"knee surgery\",\n      \"location\": \"Pune\",\n      \"policy_duration_months\": 3,\n      \"core_question\": \"coverage for knee surgery\"\n    }}\n\n    Query: \"Is maternity care covered in Mumbai for my 6 month old policy?\"\n    Output:\n    {{\n      \"procedure\": \"maternity care\",\n      \"location\": \"Mumbai\",\n      \"policy_duration_months\": 6,\n      \"core_question\": \"coverage for maternity care\"\n    }}\n\n    Now, parse the following query. Only return the JSON object, with no other text or explanations.\n\n    Query: \"{raw_query}\"\n    Output:\n    \"\"\"\n\n    # --- This section simulates making a call to the Gemini API ---\n    # In a real application, you would replace this with your API call code.\n    # For this example, we will simulate the response for the sample query.\n    \n    # This is a placeholder for the actual API call.\n    # response_text = call_gemini_api(prompt) \n    \n    # Let's simulate the expected response for our sample query\n    if \"46M\" in raw_query and \"knee surgery\" in raw_query:\n        response_text = \"\"\"\n        {\n          \"age\": 46,\n          \"gender\": \"male\",\n          \"procedure\": \"knee surgery\",\n          \"location\": \"Pune\",\n          \"policy_duration_months\": 3,\n          \"core_question\": \"coverage for knee surgery\"\n        }\n        \"\"\"\n    else:\n        response_text = '{\"error\": \"Could not parse query. Please provide a more detailed query.\"}'\n\n    # --- End of simulation ---\n\n    try:\n        # Attempt to parse the LLM's response string into a Python dictionary\n        structured_response = json.loads(response_text)\n        return structured_response\n    except json.JSONDecodeError:\n        print(\"Error: The LLM returned an invalid JSON response.\")\n        return {\"error\": \"Failed to parse LLM output.\"}\n\n# --- Step 2: Test our parser function ---\nsample_query_from_problem = \"46M, knee surgery, Pune, 3-month policy\"\nparsed_data = parse_query_with_llm(sample_query_from_problem)\n\nprint(\"\\n--- Parsed Query Data ---\")\nprint(json.dumps(parsed_data, indent=2))\nprint(\"-------------------------\")\n\n# We can now use the 'core_question' for our semantic search\nif 'core_question' in parsed_data:\n    core_question = parsed_data['core_question']\n    print(f\"\\nExtracted core question for semantic search: '{core_question}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:09:47.172440Z","iopub.execute_input":"2025-07-21T15:09:47.172806Z","iopub.status.idle":"2025-07-21T15:09:47.182698Z","shell.execute_reply.started":"2025-07-21T15:09:47.172781Z","shell.execute_reply":"2025-07-21T15:09:47.181710Z"}},"outputs":[{"name":"stdout","text":"\nParsing query with LLM: '46M, knee surgery, Pune, 3-month policy'...\n\n--- Parsed Query Data ---\n{\n  \"age\": 46,\n  \"gender\": \"male\",\n  \"procedure\": \"knee surgery\",\n  \"location\": \"Pune\",\n  \"policy_duration_months\": 3,\n  \"core_question\": \"coverage for knee surgery\"\n}\n-------------------------\n\nExtracted core question for semantic search: 'coverage for knee surgery'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# ==============================================================================\n# PART 1: LOAD ALL KNOWLEDGE BASE COMPONENTS\n# ==============================================================================\nprint(\"--- Loading Knowledge Base and Models ---\")\ntry:\n    # Load the FAISS index into a specific, non-conflicting variable name\n    faiss_index = faiss.read_index(\"policy_document_index.faiss\")\n    \n    # Load the chunk data that maps index IDs to text\n    chunks_df_map = pd.read_csv(\"policy_document_chunks.csv\")\n    \n    # Load the sentence transformer model\n    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    print(\"✅ Knowledge base and models loaded successfully.\")\n    print(f\"Index contains {faiss_index.ntotal} vectors.\")\nexcept Exception as e:\n    print(f\"Error loading files: {e}\")\n    # Define placeholder variables to prevent subsequent errors\n    faiss_index, chunks_df_map, embedding_model = None, None, None\n\n# ==============================================================================\n# PART 2: DEFINE CORE FUNCTIONS (PARSER, RETRIEVER, JUDGE)\n# ==============================================================================\n\n# --- Function 2a: LLM-Powered Query Parser ---\ndef parse_query_with_llm(raw_query):\n    print(f\"\\nParsing query with LLM: '{raw_query}'...\")\n    prompt = f\"\"\"\n    You are an expert data extraction agent...\n    Query: \"{raw_query}\"\n    Output:\n    \"\"\"\n    # This section simulates the API call\n    if \"46M\" in raw_query and \"knee surgery\" in raw_query:\n        response_text = \"\"\"\n        {\"age\": 46, \"gender\": \"male\", \"procedure\": \"knee surgery\", \"location\": \"Pune\", \"policy_duration_months\": 3, \"core_question\": \"coverage for knee surgery\"}\n        \"\"\"\n    else:\n        response_text = '{\"error\": \"Could not parse query.\"}'\n    \n    try:\n        return json.loads(response_text)\n    except json.JSONDecodeError:\n        return {\"error\": \"Failed to parse LLM output.\"}\n\n# --- Function 2b: Semantic Search Retriever (Corrected) ---\n# This function now explicitly accepts the objects it needs to work.\ndef search_documents(query, index_obj, model_obj, mapping_df, top_k=3):\n    if index_obj is None:\n        print(\"Error: FAISS index not loaded.\")\n        return None\n    print(f\"\\nSearching for: '{query}'...\")\n    query_embedding = model_obj.encode([query]).astype('float32')\n    # This will now work because we are using the passed-in 'index_obj'\n    distances, indices = index_obj.search(query_embedding, top_k)\n    results_df = mapping_df.iloc[indices[0]].copy()\n    results_df['similarity_score'] = 1 - distances[0]\n    return results_df\n\n# --- Function 2c: LLM-Powered Adjudicator (The Judge) ---\ndef get_final_decision_with_llm(parsed_query, retrieved_chunks):\n    print(\"\\nAdjudicating claim with LLM...\")\n    context = \"\"\n    # Using '_' is a safe convention for a loop variable you don't need.\n    for _, row in retrieved_chunks.iterrows():\n        context += f\"--- Relevant Clause from {row['document_name']} ---\\n{row['chunk_text']}\\n-----------------------------------------\\n\\n\"\n    prompt = f\"\"\"\n    You are an expert insurance claims adjudicator...\n    User's Situation:\n    {json.dumps(parsed_query, indent=2)}\n    Relevant Policy Clauses:\n    {context}\n    Output:\n    \"\"\"\n    # This section simulates the API call\n    response_text = \"\"\"\n    {\"decision\": \"Approved\", \"amount\": 50000.0, \"justification\": \"The user's request for knee surgery is approved...\", \"cited_clauses\": \"Section 4.2...\"}\n    \"\"\"\n    try:\n        return json.loads(response_text)\n    except json.JSONDecodeError:\n        return {\"error\": \"Failed to parse LLM output.\"}\n\n# ==============================================================================\n# PART 3: EXECUTE THE FULL END-TO-END PIPELINE\n# ==============================================================================\nif faiss_index is not None:\n    print(\"\\n--- Starting Full End-to-End Test ---\")\n    user_query = \"46M, knee surgery, Pune, 3-month policy\"\n\n    # 1. PARSE the user's query\n    parsed_data = parse_query_with_llm(user_query)\n\n    if \"error\" not in parsed_data:\n        # 2. RETRIEVE relevant documents by passing the required objects as arguments\n        core_question = parsed_data.get('core_question', user_query)\n        # We now pass our loaded objects directly to the function\n        search_results = search_documents(core_question, faiss_index, embedding_model, chunks_df_map)\n        \n        if search_results is not None:\n            # 3. JUDGE the case using the parsed data and retrieved chunks\n            final_decision = get_final_decision_with_llm(parsed_data, search_results)\n            \n            print(\"\\n\\n========================================\")\n            print(\"      FINAL DECISION REPORT\")\n            print(\"========================================\")\n            print(json.dumps(final_decision, indent=2))\n            print(\"========================================\")\nelse:\n    print(\"\\nPipeline execution skipped due to errors in loading the knowledge base.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:20:10.890117Z","iopub.execute_input":"2025-07-21T15:20:10.890550Z","iopub.status.idle":"2025-07-21T15:20:12.409162Z","shell.execute_reply.started":"2025-07-21T15:20:10.890522Z","shell.execute_reply":"2025-07-21T15:20:12.408144Z"}},"outputs":[{"name":"stdout","text":"--- Loading Knowledge Base and Models ---\n✅ Knowledge base and models loaded successfully.\nIndex contains 911 vectors.\n\n--- Starting Full End-to-End Test ---\n\nParsing query with LLM: '46M, knee surgery, Pune, 3-month policy'...\n\nSearching for: 'coverage for knee surgery'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc8b7acb4934c1d9f6a1c14ee4e7c30"}},"metadata":{}},{"name":"stdout","text":"\nAdjudicating claim with LLM...\n\n\n========================================\n      FINAL DECISION REPORT\n========================================\n{\n  \"decision\": \"Approved\",\n  \"amount\": 50000.0,\n  \"justification\": \"The user's request for knee surgery is approved...\",\n  \"cited_clauses\": \"Section 4.2...\"\n}\n========================================\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# --- Step 1: Configure Git ---\n!git config --global user.name \"Your Name\"\n!git config --global user.email \"your.email@example.com\"\n\n# --- Step 2: Save the Notebook File ---\n# This version is safer. It waits for the file to exist before moving it.\nimport time, os\nnotebook_path = '/kaggle/working/__notebook__.ipynb'\ndestination_path = '/kaggle/working/bajaj-hackrx-project/notebooks/bajaj_hackrx_pipeline.ipynb'\n\n# Wait up to 30 seconds for the notebook file to be saved by Kaggle\nfor _ in range(30):\n    if os.path.exists(notebook_path):\n        break\n    time.sleep(1)\n\nif os.path.exists(notebook_path):\n    !mv {notebook_path} {destination_path}\n    print(\"Notebook file moved successfully.\")\nelse:\n    print(\"Warning: Could not find the notebook file to move.\")\n\n# --- Step 3: Commit and Push ---\n# This makes sure we are in the right directory before running git commands\n%cd /kaggle/working/bajaj-hackrx-project\n\n!git add .\n!git commit -m \"feat: Build and test full end-to-end RAG pipeline\"\n!git push origin main\n\nprint(\"\\n✅ Successfully pushed all changes to your GitHub repository!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:29:18.681005Z","iopub.execute_input":"2025-07-21T15:29:18.682277Z","iopub.status.idle":"2025-07-21T15:29:49.554794Z","shell.execute_reply.started":"2025-07-21T15:29:18.682237Z","shell.execute_reply":"2025-07-21T15:29:49.553728Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Warning: Could not find the notebook file to move.\n[Errno 2] No such file or directory: '/kaggle/working/bajaj-hackrx-project'\n/kaggle/working\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n\n✅ Successfully pushed all changes to your GitHub repository!\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}